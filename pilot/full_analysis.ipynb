{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81343486",
   "metadata": {},
   "source": [
    "\n",
    "### Load Imports ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb6dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load Imports ###\n",
    "## include instructiosn how to instal all packages\n",
    "# Export all plots to a folder as svg and png, for each plot have an associated data file that the user can use to calculate stats\n",
    "## Add more notes throughout the notebook\n",
    "# The first time run each cell separately, then after that you can run all at once\n",
    "# In section 7, create melted vatplot where columns are variables\n",
    "# data contract: Data index is created from folder names, _ separate conditions\n",
    "# one file per cell\n",
    "# file can have more than one block\n",
    "# the data channel is 0, the stim channel is 1 across all files\n",
    "# mebrane potential is in mV, current in pA, time in ms, resistance in MOhm, capacitance in pF, conductance in nS\n",
    "# comments are input-output: io_start & io_stop, rheobase ramp: rh_start & rh_stop, chirp stimulus (subthreshold): ch_start & ch_stop\n",
    "# short-chirp stimulus (suprathreshold): sch_start & sch_stop\n",
    "# spike detection prominence is 25 for all cells, stim correction is 1000 for all cells\n",
    "\n",
    "\n",
    "# Built-in\n",
    "import os\n",
    "\n",
    "# Scientific and plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning / Stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Custom / Local\n",
    "from plots_with_stats import group_comparison_plot\n",
    "from get_index import StimEventIndexer\n",
    "from batch_process import BatchProcess\n",
    "from io_secondary_properties import (\n",
    "    get_basic_properties,\n",
    "    get_io_properties,\n",
    "    get_waveform_properties\n",
    ")\n",
    "\n",
    "# Plot parameters\n",
    "mpl.rcParams.update({'font.size': 14})\n",
    "sns.set(style=\"whitegrid\", font_scale=1.5)\n",
    "\n",
    "print('All Import Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4de5009",
   "metadata": {},
   "source": [
    "### 1. Create index file with all cells and file properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f937a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= USER SETTINGS =================\n",
    "main_path  = r\"R:\\Pantelis\\for analysis\\new patch data for analysis\"                    # Root path for data\n",
    "stim_types = ['io', 'rh', 'ch', 'sch']                                                  # Stimulation protocols\n",
    "analyzed_path    = r'R:\\Pantelis\\for analysis\\new patch data for analysis\\analyzed'     # Path to folder with analyzed data\n",
    "data_ch    = 0                                                                          # Voltage channel index\n",
    "stim_ch    = 1                                                                          # Stimulus/current channel index\n",
    "sep        = '_'                                                                        # Folder name delimiter\n",
    "output_csv = os.path.join(main_path, 'index.csv')                                       # Save location\n",
    "firing_rate_threshold = 60                                                              # Hz, threshold for excluding fast spiking cells\n",
    "stim_correction = 1000 # to make stim to pA\n",
    "spike_detection_threshold = 25 # to make spike detection prominence\n",
    "# ================================================\n",
    "\n",
    "# Build event index\n",
    "indexer = StimEventIndexer(main_path, stim_types, data_ch, stim_ch, sep)\n",
    "event_df = indexer.build_event_index()\n",
    "event_df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Summary\n",
    "total_files  = event_df['file_name'].nunique() if not event_df.empty else 0\n",
    "total_events = len(event_df)\n",
    "print(f\"üìÅ Indexed {total_files} files, found {total_events} stim events.\")\n",
    "display(event_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba000b85",
   "metadata": {},
   "source": [
    "### 2. Extract properties for all four protocols: ['io', 'rh', 'ch', 'sch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd434b9",
   "metadata": {},
   "outputs": [],
   "source": [
    " # === USER SETTINGS ===\n",
    "index_csv   = os.path.join(main_path, \"index.csv\")\n",
    "analyzed_dir = os.path.join(main_path, 'analyzed')\n",
    "njobs       = 1\n",
    "# =====================\n",
    "\n",
    "idx_df = pd.read_csv(index_csv)\n",
    "processor = BatchProcess(\n",
    "    main_path, idx_df,\n",
    "    njobs=njobs,\n",
    "    stim_correction=stim_correction,\n",
    "    prominence=spike_detection_threshold\n",
    ")\n",
    "processor.run_all(analyzed_dir, stim_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd42aaae",
   "metadata": {},
   "source": [
    "### 3. Extract IO secondary properties (ie, waveform metrics and i-o slope, percent firing rate etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050196a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= USER CONFIGURATION =================\n",
    "stim_type    = 'io'\n",
    "group_cols   = ['treatment', 'cell_id']\n",
    "# =====================================================\n",
    "\n",
    "# ===== Load extracted features =====\n",
    "basic_path = os.path.join(analyzed_path, f\"{stim_type}_basic/features.csv\")\n",
    "wave_path  = os.path.join(analyzed_path, f\"{stim_type}_wave/features.csv\")\n",
    "df_basic   = pd.read_csv(basic_path)\n",
    "df_wave    = pd.read_csv(wave_path)\n",
    "\n",
    "# ===== Standardize column names =====\n",
    "df_basic = df_basic.rename(columns={'file_name': 'cell_id', 'condition_0': 'treatment'})\n",
    "df_wave  = df_wave.rename(columns={'file_name': 'cell_id', 'condition_0': 'treatment'})\n",
    "\n",
    "# ===== Filter out non-spiking cells =====\n",
    "spike_summary = df_basic.groupby('cell_id')['spike_frequency'].max().reset_index()\n",
    "non_spiking = spike_summary[spike_summary['spike_frequency'] <= 0]['cell_id']\n",
    "df_basic = df_basic[~df_basic['cell_id'].isin(non_spiking)].reset_index(drop=True)\n",
    "df_wave  = df_wave[~df_wave['cell_id'].isin(non_spiking)].reset_index(drop=True)\n",
    "\n",
    "# ===== Basic properties (RMP, Rin) =====\n",
    "plot_basic = (\n",
    "    df_basic.groupby(group_cols)[['input_resistance', 'rmp']]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "plot_basic.loc[plot_basic['input_resistance'] < 0, 'input_resistance'] = np.NaN\n",
    "basic_properties = get_basic_properties(plot_basic, group_cols)\n",
    "\n",
    "# ===== IO curve properties (freq, slope, rheobase) =====\n",
    "plot_io = (\n",
    "    df_basic.groupby(group_cols + ['amp'])[['spike_frequency']]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "plot_io = plot_io[plot_io['amp'] > 0]\n",
    "io_properties = get_io_properties(plot_io, group_cols, show_plot=False)\n",
    "\n",
    "# ===== Waveform properties (AP shape) =====\n",
    "df_wave = df_wave.dropna(subset=['mV', 'time'])\n",
    "waveform_properties = get_waveform_properties(df_wave, group_cols, show_plot=False)\n",
    "\n",
    "# ===== Save outputs separately ====\n",
    "summary_basic_io = io_properties.merge(basic_properties, on=group_cols, how='left')\n",
    "summary_basic_io.to_csv(os.path.join(analyzed_path, f'{stim_type}_basic', 'summary_io.csv'), index=False)\n",
    "waveform_properties.to_csv(os.path.join(analyzed_path, f'{stim_type}_wave', 'summary_waveform.csv'), index=False)\n",
    "\n",
    "# ===== Save filtered feature tables without overwriting original =====\n",
    "df_basic.to_csv(os.path.join(analyzed_path, f'{stim_type}_basic', 'features_filtered.csv'), index=False)\n",
    "df_wave.to_csv(os.path.join(analyzed_path, f'{stim_type}_wave', 'features_filtered.csv'), index=False)\n",
    "\n",
    "print(\"‚úÖ Property extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e11c6a",
   "metadata": {},
   "source": [
    "### 4. Find cells with bad responses and high firing rate to exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eed8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER CONFIGURATION\n",
    "group_col = 'treatment'\n",
    "palette = ['#1f77b4', '#ff7f0e']\n",
    "\n",
    "# Load cell metadata\n",
    "path_basic_io = os.path.join(analyzed_path, 'io_basic', 'summary_io.csv')\n",
    "df_basic_io = pd.read_csv(path_basic_io)\n",
    "\n",
    "# Combine\n",
    "cells_with_bad_responses = set([]) # placeholder if we want to manually add any cells to exclude\n",
    "# Identify fast spiking cells for exclusion\n",
    "fast_spiking = set(df_basic_io[df_basic_io['max_firing_rate'] > firing_rate_threshold]['cell_id'])\n",
    "to_exclude = cells_with_bad_responses.union(fast_spiking)\n",
    "\n",
    "# Summary per treatment\n",
    "print(f\"üîç Found {len(cells_with_bad_responses)} cells with bad responses.\")\n",
    "print(f\"‚ö° Found {len(fast_spiking)} fast spiking cells.\")\n",
    "print(f\"‚ùå Total cells to exclude: {len(to_exclude)}\")\n",
    "\n",
    "print(\"\\nüìä Cell counts by treatment BEFORE exclusion:\")\n",
    "print(df_basic_io.groupby('treatment')['cell_id'].nunique())\n",
    "\n",
    "remaining = df_basic_io[~df_basic_io['cell_id'].isin(to_exclude)]\n",
    "print(\"\\n‚úÖ Remaining cells per treatment AFTER exclusion:\")\n",
    "print(remaining.groupby('treatment')['cell_id'].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69140cbb",
   "metadata": {},
   "source": [
    "### 5. I-O Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b13515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== PLOT: IO Spike Frequency vs Amp ====\n",
    "df_basic = pd.read_csv(os.path.join(analyzed_path, 'io_basic', 'features_filtered.csv'))\n",
    "df_basic = df_basic[~df_basic['cell_id'].isin(to_exclude)]\n",
    "sns.relplot(\n",
    "    data=df_basic,\n",
    "    x='amp', y='spike_frequency',\n",
    "    marker='o', hue='treatment',\n",
    "    kind='line', errorbar='se',\n",
    "    palette='tab10',\n",
    "    height=5, aspect=1.3\n",
    ")\n",
    "plt.title('IO: Spike Frequency vs Current Amplitude')\n",
    "plt.xlabel('Current Injection (pA)')\n",
    "plt.ylabel('Spike Frequency (Hz)')\n",
    "\n",
    "# ==== PLOT: Rheobase (RH) ====\n",
    "df_rh = pd.read_csv(os.path.join(analyzed_path, 'rh', 'features.csv'))\n",
    "df_rh = df_rh.rename(columns={'file_name': 'cell_id', 'condition_0': 'treatment'})\n",
    "df_rh = df_rh[~df_rh['cell_id'].isin(to_exclude)]\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.barplot(data=df_rh, x='treatment', y='rheobase', palette='pastel', errorbar='se')\n",
    "sns.stripplot(data=df_rh, x='treatment', y='rheobase', color='black', jitter=True, alpha=0.6)\n",
    "plt.title('Rheobase by Treatment (RH)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb49918",
   "metadata": {},
   "source": [
    "### 6. AP Waveform (Standard Deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8bce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wave  = pd.read_csv(os.path.join(analyzed_path, 'io_wave', 'features_filtered.csv'))\n",
    "df_wave = df_wave[~df_wave['cell_id'].isin(to_exclude)]\n",
    "# ==== PLOT: IO Waveform Trace ====\n",
    "sns.relplot(\n",
    "    data=df_wave,\n",
    "    x='time', y='mV',\n",
    "    hue='treatment', kind='line',\n",
    "    estimator=np.mean, errorbar='sd',\n",
    "    palette='tab10',\n",
    "    height=5, aspect=1.3\n",
    ")\n",
    "plt.title('IO: Averaged Waveform by Treatment')\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Membrane Potential (mV)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b45448",
   "metadata": {},
   "source": [
    "### 7. IO and Waveform Properties - Summary Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd0258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load to ensure no prior modification affects the input\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df_basic_io = pd.read_csv(os.path.join(analyzed_path, 'io_basic', 'summary_io.csv'))\n",
    "df_waveform = pd.read_csv(os.path.join(analyzed_path, 'io_wave', 'summary_waveform.csv'))\n",
    "\n",
    "# Apply exclusions\n",
    "df_basic_io = df_basic_io[~df_basic_io['cell_id'].isin(to_exclude)]\n",
    "df_waveform = df_waveform[~df_waveform['cell_id'].isin(to_exclude)]\n",
    "\n",
    "# Define column sets\n",
    "basic_io_cols = [\n",
    "    'fr_at_20_percent_input', 'fr_at_40_percent_input', 'fr_at_60_percent_input',\n",
    "    'fr_at_80_percent_input', 'fr_at_max_input', 'i_amp_at_half_max_fr',\n",
    "    'input_resistance', 'resting_membrane_potential', 'max_firing_rate',\n",
    "    'rheobase', 'io_slope'\n",
    "]\n",
    "\n",
    "waveform_cols = [\n",
    "    'ap_peak', 'threshold', 'ahp', 'peak_to_trough', 'rise_time', 'half_width'\n",
    "]\n",
    "\n",
    "## add seaborn plots for 3 categories: basic io, waveform, rh\n",
    "# melt and plot all variables in basic_io_cols and waveform_cols\n",
    "\n",
    "sns.catplot(\n",
    "    data=df_basic_io,\n",
    "    x='treatment', y='io_slope',\n",
    "    kind='bar', errorbar='se', palette='pastel',\n",
    "    height=5, aspect=1\n",
    ")\n",
    "\n",
    "\n",
    "# # Run and plot\n",
    "# print(\"üìä Analyzing IO features...\")\n",
    "# res_basic_io = group_comparison_plot(\n",
    "#     df_basic_io,\n",
    "#     group_column='treatment',\n",
    "#     dependent_variables=basic_io_cols,\n",
    "#     palette=palette,\n",
    "#     n_cols=4\n",
    "# )\n",
    "\n",
    "# print(\"üìä Analyzing waveform features...\")\n",
    "# res_waveform = group_comparison_plot(\n",
    "#     df_waveform,\n",
    "#     group_column='treatment',\n",
    "#     dependent_variables=waveform_cols,\n",
    "#     palette=palette,\n",
    "#     n_cols=3\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5bc7b9",
   "metadata": {},
   "source": [
    "### 8. PCA - Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728a9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== COMBINE, PCA + GMM CLUSTERING & PLOT ====\n",
    "# 1) merge on cell_id & treatment\n",
    "df_merged = pd.merge(\n",
    "    df_basic_io, df_waveform,\n",
    "    on=['cell_id','treatment'], how='inner'\n",
    ")\n",
    "df_merged = df_merged.fillna(df_merged.median(numeric_only=True))\n",
    "\n",
    "# 2) pick features and standardize\n",
    "features = basic_io_cols + waveform_cols\n",
    "X = df_merged[features].values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_merged[features].values)\n",
    "y = df_merged[group_col].values\n",
    "\n",
    "# 2. PCA to 2D for visualization & classification\n",
    "pca = PCA(n_components=2, random_state=0)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# 3. Stratified K-Fold Logistic Regression\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "log_reg = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "scores = []\n",
    "\n",
    "for train_idx, test_idx in skf.split(X_pca, y):\n",
    "    X_train, X_test = X_pca[train_idx], X_pca[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    log_reg.fit(X_train, y_train)\n",
    "    y_pred = log_reg.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    scores.append(acc)\n",
    "\n",
    "print(f\"‚úÖ Mean CV Accuracy: {np.mean(scores):.3f}\")\n",
    "print(f\"üìÑ Fold Accuracies:  {np.round(scores, 3)}\")\n",
    "\n",
    "# 4. Final model on full PCA data\n",
    "log_reg.fit(X_pca, y)\n",
    "\n",
    "# 5. Decision boundary plot\n",
    "x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\n",
    "y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(x_min, x_max, 500),\n",
    "    np.linspace(y_min, y_max, 500)\n",
    ")\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = log_reg.predict_proba(grid)[:, 1].reshape(xx.shape)\n",
    "\n",
    "# 6. Scatter + decision boundary\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "for tr, col in zip(np.unique(y), palette):\n",
    "    mask = y == tr\n",
    "    ax.scatter(X_pca[mask, 0], X_pca[mask, 1], label=tr, c=col, edgecolor='k', s=50)\n",
    "\n",
    "ax.contour(xx, yy, Z, levels=[0.5], linestyles='--', colors='gray')\n",
    "ax.set_xlabel('PC 1')\n",
    "ax.set_ylabel('PC 2')\n",
    "ax.set_title(f'PCA + Logistic Regression\\n5-Fold CV Accuracy: {np.mean(scores):.2f}')\n",
    "ax.legend(title=group_col)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5b2c54",
   "metadata": {},
   "source": [
    "### 9. Chrip (subthreshold resonance) and Short Chirp (suprathreshold spike transfer) normalize and break into bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e30367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup ===\n",
    "stim_types = ['ch', 'sch']\n",
    "metrics = {'ch': 'impedance', 'sch': 'spike_count'}\n",
    "file_paths = {s: os.path.join(analyzed_path, s, 'features.csv') for s in stim_types}\n",
    "freq_bins = (3, 6, 12, 15, 30, 60)\n",
    "\n",
    "# === Requires `to_exclude` to be defined already in the notebook ===\n",
    "\n",
    "# === Delta function ===\n",
    "def compute_deltas(group, y):\n",
    "    bp = group.set_index('freq_bins')[y]\n",
    "    return pd.Series({\n",
    "        'delta_3-6__6-12':   bp.get(pd.Interval(3, 6),  np.nan) - bp.get(pd.Interval(6, 12), np.nan),\n",
    "        'delta_15-30__3-6':  bp.get(pd.Interval(15, 30), np.nan) - bp.get(pd.Interval(3, 6), np.nan),\n",
    "        'delta_15-30__6-12': bp.get(pd.Interval(15, 30), np.nan) - bp.get(pd.Interval(6, 12), np.nan),\n",
    "    })\n",
    "\n",
    "# === Storage containers ===\n",
    "dfs = {}                 # full df with raw and norm columns\n",
    "dfs_binned_raw = {}      # grouped raw\n",
    "dfs_binned_norm = {}     # grouped norm\n",
    "deltas_raw = {}\n",
    "deltas_norm = {}\n",
    "\n",
    "# === Process CH and SCH ===\n",
    "for stim in stim_types:\n",
    "    metric = metrics[stim]\n",
    "    \n",
    "    # Load and clean\n",
    "    df = pd.read_csv(file_paths[stim])\n",
    "    df = df.rename(columns={'file_name': 'cell_id', 'condition_0': 'treatment'})\n",
    "    df = df[~df['cell_id'].isin(to_exclude)].copy()\n",
    "\n",
    "    # Remove zero-spike cells if needed\n",
    "    if metric == 'spike_count':\n",
    "        total = df.groupby(['cell_id', 'treatment'])[metric].transform('sum')\n",
    "        df = df[total > 0]\n",
    "\n",
    "    # Normalize\n",
    "    total = df.groupby(['cell_id', 'treatment'])[metric].transform('sum')\n",
    "    norm_col = f'norm_{metric}'\n",
    "    df[norm_col] = df[metric] / total\n",
    "\n",
    "    # 1) Check sums of norm values per cell√ótreatment\n",
    "    check = (\n",
    "        df\n",
    "        .groupby(['cell_id', 'treatment'])[norm_col]\n",
    "        .sum()\n",
    "        .reset_index(name='sum_norm')\n",
    "    )\n",
    "\n",
    "    # 2) Print summaries: should all be (approximately) 1\n",
    "    print(f\"--- {stim.upper()} normalization check ---\")\n",
    "    print(f\"Min sum: {check['sum_norm'].min():.4f}, Max sum: {check['sum_norm'].max():.4f}\")\n",
    "    print(f\"Any deviating from 1? {((check['sum_norm'] < 0.999) | (check['sum_norm'] > 1.001)).any()}\")\n",
    "    print(check.head(), \"\\n\")\n",
    "\n",
    "    # Bin frequencies\n",
    "    df['freq_bins'] = pd.cut(df['freq'], freq_bins)\n",
    "\n",
    "    # Save full df\n",
    "    dfs[stim] = df\n",
    "\n",
    "    # === Binned RAW ===\n",
    "    df_binned_raw = (\n",
    "        df.groupby(['freq_bins', 'cell_id', 'treatment'], observed=True)[metric]\n",
    "        .mean().reset_index()\n",
    "    )\n",
    "    dfs_binned_raw[stim] = df_binned_raw\n",
    "\n",
    "    deltas_raw[stim] = (\n",
    "        df_binned_raw.groupby(['cell_id', 'treatment'])\n",
    "        .apply(compute_deltas, y=metric)\n",
    "        .reset_index()\n",
    "        .melt(id_vars=['cell_id', 'treatment'], var_name='delta_type', value_name=metric)\n",
    "    )\n",
    "\n",
    "    # === Binned NORM ===\n",
    "    df_binned_norm = (\n",
    "        df.groupby(['freq_bins', 'cell_id', 'treatment'], observed=True)[norm_col]\n",
    "        .mean().reset_index()\n",
    "    )\n",
    "    dfs_binned_norm[stim] = df_binned_norm\n",
    "\n",
    "    deltas_norm[stim] = (\n",
    "        df_binned_norm.groupby(['cell_id', 'treatment'])\n",
    "        .apply(compute_deltas, y=norm_col)\n",
    "        .reset_index()\n",
    "        .melt(id_vars=['cell_id', 'treatment'], var_name='delta_type', value_name=metric)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f129ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stim in stim_types:\n",
    "    metric = metrics[stim]\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.lineplot(data=dfs[stim], x='freq', y=metric, hue='treatment', errorbar='se', marker='o')\n",
    "    plt.title(f'{stim.upper()}: Raw {metric.capitalize()} vs Frequency')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel(metric)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2165594e",
   "metadata": {},
   "source": [
    "### 10. Plot Normalized (normalized to sum impedance or spike counts per cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae41d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stim in stim_types:\n",
    "    norm_col = f'norm_{metrics[stim]}'\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.lineplot(data=dfs[stim], x='freq', y=norm_col, hue='treatment', errorbar='se', marker='o')\n",
    "    plt.title(f'{stim.upper()}: Normalized {metrics[stim].capitalize()} vs Frequency')\n",
    "    plt.xlabel('Frequency (Hz)')\n",
    "    plt.ylabel(norm_col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1296148c",
   "metadata": {},
   "source": [
    "### 11. Divide to Frequency Bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039d4594",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stim in stim_types:\n",
    "    metric = metrics[stim]\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.barplot(data=dfs_binned_raw[stim], x='freq_bins', y=metric, hue='treatment', errorbar='se')\n",
    "    plt.title(f'{stim.upper()}: Binned Raw {metric.capitalize()}')\n",
    "    plt.xlabel('Frequency Band (Hz)')\n",
    "    plt.ylabel(metric)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for stim in stim_types:\n",
    "    norm_col = f'norm_{metrics[stim]}'\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.barplot(data=dfs_binned_norm[stim], x='freq_bins', y=norm_col, hue='treatment', errorbar='se')\n",
    "    plt.title(f'{stim.upper()}: Binned Normalized {metrics[stim].capitalize()}')\n",
    "    plt.xlabel('Frequency Band (Hz)')\n",
    "    plt.ylabel(norm_col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0837445",
   "metadata": {},
   "source": [
    "### 12. Deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f249c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stim in stim_types:\n",
    "    metric = metrics[stim]\n",
    "    plt.figure(figsize=(7.5, 5))\n",
    "    sns.barplot(data=deltas_raw[stim], x='delta_type', y=metric, hue='treatment', errorbar='se')\n",
    "    plt.title(f'{stim.upper()}: Œîs on Raw {metric.capitalize()}')\n",
    "    plt.xlabel('Delta Type')\n",
    "    plt.ylabel(metric)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for stim in stim_types:\n",
    "    metric = metrics[stim]\n",
    "    plt.figure(figsize=(7.5, 5))\n",
    "    sns.barplot(data=deltas_norm[stim], x='delta_type', y=metric, hue='treatment', errorbar='se')\n",
    "    plt.title(f'{stim.upper()}: Œîs on Normalized {metric.capitalize()}')\n",
    "    plt.xlabel('Delta Type')\n",
    "    plt.ylabel(metric)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
