{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3266522",
   "metadata": {},
   "source": [
    "# Main Pipeline for Single-cell IC analysis (IO / RH / CH / SCH)\n",
    "---\n",
    "## How to Run\n",
    "1) Install packages (one-liner)\n",
    "   ```bash\n",
    "   pip install numpy pandas matplotlib seaborn scikit-learn\n",
    "   ```\n",
    "   *(Your project also uses `plots_with_stats`, `get_index`, `batch_process`, `io_secondary_properties` ‚Äî keep them alongside this script.)*\n",
    "2) Put your data under **main_path** (see below). The **data contract** is enforced upstream by your code:\n",
    "   - **Folder index** is derived from folder names; `_` splits conditions.\n",
    "   - **One file per cell**; a file **can have more than one block**.\n",
    "   - **Channels**: data (Vm) = **0**, stim = **1**.\n",
    "   - **Units**: Vm **mV**, I **pA**, time **ms**, Rin **MŒ©**, C **pF**, G **nS**.\n",
    "   - **Tags**: `io_start/io_stop`, `rh_start/rh_stop`, `ch_start/ch_stop`, `sch_start/sch_stop`.\n",
    "   - **Spike prominence** = **25** for all cells.\n",
    "   - **Stim correction** = **1000** for all cells.\n",
    "3) **First run:** execute cells **one-by-one** (this script creates output folders and a first-run flag). After that, you can **Run All**.\n",
    "4) **Outputs** (created under `FIG_ROOT` and `EXPORT_ROOT`):\n",
    "   - Every plot is exported as **SVG + PNG** and the **exact plotted data as CSV**.\n",
    "   - A small meta JSON is also written per figure with parameters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598040e",
   "metadata": {},
   "source": [
    "### Load Imports ###\n",
    "*(Kept minimal; analysis logic unchanged)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e87bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Scientific and plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning / Stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Custom / Local (unchanged)\n",
    "from plots_with_stats import group_comparison_plot\n",
    "from get_index import StimEventIndexer\n",
    "from batch_process import BatchProcess\n",
    "from io_secondary_properties import (\n",
    "    get_basic_properties,\n",
    "    get_io_properties,\n",
    "    get_waveform_properties\n",
    ")\n",
    "\n",
    "# Plot parameters\n",
    "mpl.rcParams.update({'font.size': 14})\n",
    "sns.set(style=\"whitegrid\", font_scale=1.5)\n",
    "\n",
    "print('All Imports Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e27ead",
   "metadata": {},
   "source": [
    "## Central Config & First-Run Guard\n",
    "- Keep your existing variables; only add figure/export roots and a helper flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f52387b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ================= USER SETTINGS =================\n",
    "main_path  = r\"R:\\Pantelis\\for analysis\\new patch data for analysis\"                    # Root path for data\n",
    "stim_types = ['io', 'rh', 'ch', 'sch']                                                  # Stimulation protocols\n",
    "analyzed_path    = r'R:\\Pantelis\\for analysis\\new patch data for analysis\\analyzed'     # Folder with analyzed data\n",
    "data_ch    = 0                                                                          # Voltage channel index\n",
    "stim_ch    = 1                                                                          # Stimulus/current channel index\n",
    "sep        = '_'                                                                        # Folder name delimiter\n",
    "output_csv = os.path.join(main_path, 'index.csv')                                       # Index save location\n",
    "firing_rate_threshold = 60                                                              # Hz, threshold for excluding fast spiking cells\n",
    "stim_correction = 1000                                                                  # to make stim to pA\n",
    "spike_detection_threshold = 25                                                          # spike detection prominence\n",
    "njobs       = 1                                                                         # Number of parallel jobs\n",
    "# ================================================\n",
    "\n",
    "# Figure/Export roots (added)\n",
    "FIG_ROOT = Path(analyzed_path) / 'figures'\n",
    "EXPORT_ROOT = Path(analyzed_path) / 'exports'\n",
    "FIG_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "EXPORT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# First-run flag\n",
    "FIRST_RUN_FLAG = Path(analyzed_path) / '.first_run_complete'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc7574c",
   "metadata": {},
   "source": [
    "## Save Helpers (minimal, reused everywhere)\n",
    "Every plot saves: **.png**, **.svg**, **.csv**, and optional **.meta.json** under `figures/<section>/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b327226f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def _ensure_section(section: str) -> Path:\n",
    "    p = FIG_ROOT / section\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "\n",
    "def save_plot_and_data(section: str, name: str, df: pd.DataFrame, fig: plt.Figure, params: dict | None = None):\n",
    "    sec = _ensure_section(section)\n",
    "    base = sec / name\n",
    "    # CSV of the plotted data\n",
    "    if df is not None:\n",
    "        df.to_csv(f\"{base}.csv\", index=False)\n",
    "    # Meta\n",
    "    if params:\n",
    "        with open(f\"{base}.meta.json\", \"w\") as f:\n",
    "            json.dump(params, f, indent=2, default=str)\n",
    "    # Figures\n",
    "    fig.savefig(f\"{base}.png\", dpi=150, bbox_inches='tight')\n",
    "    fig.savefig(f\"{base}.svg\", bbox_inches='tight')\n",
    "    # plt.close(fig)\n",
    "\n",
    "\n",
    "def stamp_run_metadata():\n",
    "    meta = {\n",
    "        'timestamp': datetime.now().isoformat(timespec='seconds'),\n",
    "        'main_path': str(main_path),\n",
    "        'analyzed_path': str(analyzed_path),\n",
    "        'stim_types': stim_types,\n",
    "        'stim_correction': stim_correction,\n",
    "        'spike_detection_threshold': spike_detection_threshold,\n",
    "    }\n",
    "    with open(EXPORT_ROOT / 'run_metadata.json', 'w') as f:\n",
    "        json.dump(meta, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d773ffcd",
   "metadata": {},
   "source": [
    "## First Run\n",
    "- Creates folders, writes metadata, and sets a flag. **Run this cell on the very first pass.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ef0b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StimEventIndexer(main_path, stim_types, data_ch, stim_ch, sep)\n",
    "event_df = indexer.build_event_index()\n",
    "event_df.to_csv(output_csv, index=False)\n",
    "\n",
    "# Summary\n",
    "total_files  = event_df['file_name'].nunique() if not event_df.empty else 0\n",
    "total_events = len(event_df)\n",
    "print(f\"üìÅ Indexed {total_files} files, found {total_events} stim events.\")\n",
    "try:\n",
    "    display(event_df)\n",
    "except Exception:\n",
    "    print(event_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e962875a",
   "metadata": {},
   "source": [
    "### 2. Extract properties for all four protocols: ['io', 'rh', 'ch', 'sch']\n",
    "*(Logic unchanged; ensures `stim_correction` and `prominence` are passed.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded83288",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_csv   = os.path.join(main_path, \"index.csv\")\n",
    "analyzed_dir = os.path.join(main_path, 'analyzed')\n",
    "\n",
    "idx_df = pd.read_csv(index_csv)\n",
    "processor = BatchProcess(\n",
    "   main_path, idx_df,\n",
    "   njobs=njobs,\n",
    "   stim_correction=stim_correction,\n",
    "   prominence=spike_detection_threshold\n",
    ")\n",
    "processor.run_all(analyzed_dir, stim_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41cf63c",
   "metadata": {},
   "source": [
    "### 3. Extract IO secondary properties (waveform metrics, I‚ÄìO slope, etc.)\n",
    "*(Logic unchanged; adds saving of summary CSVs ‚Äî you already do ‚Äî and notes.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477259fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= USER CONFIGURATION =================\n",
    "stim_type    = 'io'\n",
    "group_cols   = ['treatment', 'cell_id']\n",
    "# =====================================================\n",
    "\n",
    "# ===== Load extracted features =====\n",
    "basic_path = os.path.join(analyzed_path, f\"{stim_type}_basic/features.csv\")\n",
    "wave_path  = os.path.join(analyzed_path, f\"{stim_type}_wave/features.csv\")\n",
    "df_basic   = pd.read_csv(basic_path)\n",
    "df_wave    = pd.read_csv(wave_path)\n",
    "\n",
    "# ===== Standardize column names =====\n",
    "df_basic = df_basic.rename(columns={'file_name': 'cell_id', 'condition_0': 'treatment'})\n",
    "df_wave  = df_wave.rename(columns={'file_name': 'cell_id', 'condition_0': 'treatment'})\n",
    "\n",
    "# ===== Filter out non-spiking cells =====\n",
    "spike_summary = df_basic.groupby('cell_id')['spike_frequency'].max().reset_index()\n",
    "non_spiking = spike_summary[spike_summary['spike_frequency'] <= 0]['cell_id']\n",
    "df_basic = df_basic[~df_basic['cell_id'].isin(non_spiking)].reset_index(drop=True)\n",
    "df_wave  = df_wave[~df_wave['cell_id'].isin(non_spiking)].reset_index(drop=True)\n",
    "\n",
    "# ===== Basic properties (RMP, Rin) =====\n",
    "plot_basic = (\n",
    "    df_basic.groupby(group_cols)[['input_resistance', 'rmp']]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "plot_basic.loc[plot_basic['input_resistance'] < 0, 'input_resistance'] = np.NaN\n",
    "basic_properties = get_basic_properties(plot_basic, group_cols)\n",
    "\n",
    "# ===== IO curve properties (freq, slope, rheobase) =====\n",
    "plot_io = (\n",
    "    df_basic.groupby(group_cols + ['amp'])[['spike_frequency']]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "plot_io = plot_io[plot_io['amp'] > 0]\n",
    "io_properties = get_io_properties(plot_io, group_cols, show_plot=False)\n",
    "\n",
    "# ===== Waveform properties (AP shape) =====\n",
    "df_wave = df_wave.dropna(subset=['mV', 'time'])\n",
    "waveform_properties = get_waveform_properties(df_wave, group_cols, show_plot=False)\n",
    "\n",
    "# ===== Save outputs separately (unchanged) ====\n",
    "summary_basic_io = io_properties.merge(basic_properties, on=group_cols, how='left')\n",
    "summary_basic_io.to_csv(os.path.join(analyzed_path, f'{stim_type}_basic', 'summary_io.csv'), index=False)\n",
    "waveform_properties.to_csv(os.path.join(analyzed_path, f'{stim_type}_wave', 'summary_waveform.csv'), index=False)\n",
    "\n",
    "# ===== Save filtered feature tables without overwriting original =====\n",
    "df_basic.to_csv(os.path.join(analyzed_path, f'{stim_type}_basic', 'features_filtered.csv'), index=False)\n",
    "df_wave.to_csv(os.path.join(analyzed_path, f'{stim_type}_wave', 'features_filtered.csv'), index=False)\n",
    "\n",
    "print(\"‚úÖ Property extraction complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62732b99",
   "metadata": {},
   "source": [
    "### 4. Find cells with bad responses and high firing rate to exclude\n",
    "*(Logic unchanged; we also export the exclusion list.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER CONFIGURATION\n",
    "group_col = 'treatment'\n",
    "palette = ['#1f77b4', '#ff7f0e']\n",
    "\n",
    "# Load cell metadata\n",
    "path_basic_io = os.path.join(analyzed_path, 'io_basic', 'summary_io.csv')\n",
    "df_basic_io = pd.read_csv(path_basic_io)\n",
    "\n",
    "# Combine\n",
    "cells_with_bad_responses = set([]) # manual additions if needed\n",
    "fast_spiking = set(df_basic_io[df_basic_io['max_firing_rate'] > firing_rate_threshold]['cell_id'])\n",
    "to_exclude = cells_with_bad_responses.union(fast_spiking)\n",
    "\n",
    "# Export exclusions\n",
    "pd.DataFrame({'cell_id': sorted(to_exclude)}).to_csv(EXPORT_ROOT / 'excluded_cells.csv', index=False)\n",
    "\n",
    "# Summary per treatment\n",
    "print(f\"üîç Found {len(cells_with_bad_responses)} cells with bad responses.\")\n",
    "print(f\"‚ö° Found {len(fast_spiking)} fast spiking cells.\")\n",
    "print(f\"‚ùå Total cells to exclude: {len(to_exclude)}\")\n",
    "\n",
    "print(\"\\nüìä Cell counts by treatment BEFORE exclusion:\")\n",
    "print(df_basic_io.groupby('treatment')['cell_id'].nunique())\n",
    "\n",
    "remaining = df_basic_io[~df_basic_io['cell_id'].isin(to_exclude)]\n",
    "print(\"\\n‚úÖ Remaining cells per treatment AFTER exclusion:\")\n",
    "print(remaining.groupby('treatment')['cell_id'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e99be",
   "metadata": {},
   "source": [
    "### 5. I‚ÄìO Plot & Rheobase (save figures + CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bdaa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== IO Spike Frequency vs Amp ====\n",
    "df_basic_filt = pd.read_csv(os.path.join(analyzed_path, 'io_basic', 'features_filtered.csv'))\n",
    "df_basic_filt = df_basic_filt[~df_basic_filt['cell_id'].isin(to_exclude)]\n",
    "\n",
    "# build plotted data (mean ¬± se by treatment √ó amp)\n",
    "io_plot_df = (\n",
    "    df_basic_filt.groupby(['treatment','amp'], as_index=False)['spike_frequency']\n",
    "    .agg(mean='mean', sem=lambda x: x.std(ddof=1)/np.sqrt(max(len(x),1)))\n",
    ")\n",
    "\n",
    "# draw\n",
    "g = sns.relplot(\n",
    "    data=df_basic_filt,\n",
    "    x='amp', y='spike_frequency',\n",
    "    marker='o', hue='treatment',\n",
    "    kind='line', errorbar='se',\n",
    "    palette='tab10',\n",
    "    height=5, aspect=1.3\n",
    ")\n",
    "g.set_axis_labels('Current Injection (pA)', 'Spike Frequency (Hz)')\n",
    "g.fig.suptitle('IO: Spike Frequency vs Current Amplitude')\n",
    "plt.tight_layout()\n",
    "# save figure and exact data used (use raw rows for transparency)\n",
    "save_plot_and_data('io', 'io_fi_curve', df_basic_filt.copy(), g.fig, {'note':'relplot with errorbar=se'})\n",
    "\n",
    "# ==== Rheobase (RH) ====\n",
    "df_rh = pd.read_csv(os.path.join(analyzed_path, 'rh', 'features.csv'))\n",
    "df_rh = df_rh.rename(columns={'file_name': 'cell_id', 'condition_0': 'treatment'})\n",
    "df_rh = df_rh[~df_rh['cell_id'].isin(to_exclude)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "sns.barplot(data=df_rh, x='treatment', y='rheobase', palette='pastel', errorbar='se', ax=ax)\n",
    "sns.stripplot(data=df_rh, x='treatment', y='rheobase', color='black', jitter=True, alpha=0.6, ax=ax)\n",
    "ax.set_title('Rheobase by Treatment (RH)')\n",
    "ax.set_xlabel('Treatment')\n",
    "ax.set_ylabel('Rheobase (pA)')\n",
    "plt.tight_layout()\n",
    "save_plot_and_data('rh', 'rheobase_by_treatment', df_rh.copy(), fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b410d8",
   "metadata": {},
   "source": [
    "### 6. AP Waveform (SD) ‚Äî Save figure + CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d7c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wave_filt  = pd.read_csv(os.path.join(analyzed_path, 'io_wave', 'features_filtered.csv'))\n",
    "df_wave_filt = df_wave_filt[~df_wave_filt['cell_id'].isin(to_exclude)]\n",
    "\n",
    "g = sns.relplot(\n",
    "    data=df_wave_filt,\n",
    "    x='time', y='mV',\n",
    "    hue='treatment', kind='line',\n",
    "    estimator=np.mean, errorbar='sd',\n",
    "    palette='tab10',\n",
    "    height=5, aspect=1.3\n",
    ")\n",
    "g.set_axis_labels('Time (ms)', 'Membrane Potential (mV)')\n",
    "g.fig.suptitle('IO: Averaged Waveform by Treatment')\n",
    "plt.tight_layout()\n",
    "save_plot_and_data('io_wave', 'avg_waveform_by_treatment', df_wave_filt[['time','mV','treatment','cell_id']].copy(), g.fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddc56a4",
   "metadata": {},
   "source": [
    "### 7. IO & Waveform Properties ‚Äî **Melted catplot** (columns ‚Üí variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a2cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load summaries to avoid carryover\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df_basic_io = pd.read_csv(os.path.join(analyzed_path, 'io_basic', 'summary_io.csv'))\n",
    "df_waveform = pd.read_csv(os.path.join(analyzed_path, 'io_wave', 'summary_waveform.csv'))\n",
    "\n",
    "# Apply exclusions\n",
    "df_basic_io = df_basic_io[~df_basic_io['cell_id'].isin(to_exclude)]\n",
    "df_waveform = df_waveform[~df_waveform['cell_id'].isin(to_exclude)]\n",
    "\n",
    "# Column sets (unchanged)\n",
    "basic_io_cols = [\n",
    "    'fr_at_20_percent_input', 'fr_at_40_percent_input', 'fr_at_60_percent_input',\n",
    "    'fr_at_80_percent_input', 'fr_at_max_input', 'i_amp_at_half_max_fr',\n",
    "    'input_resistance', 'resting_membrane_potential', 'max_firing_rate',\n",
    "    'rheobase', 'io_slope'\n",
    "]\n",
    "\n",
    "waveform_cols = [\n",
    "    'ap_peak', 'threshold', 'ahp', 'peak_to_trough', 'rise_time', 'half_width'\n",
    "]\n",
    "\n",
    "all_df = pd.merge(df_basic_io, df_waveform, on=['cell_id'], how='outer', suffixes=('_basic','_wave'))\n",
    "\n",
    "# Melt: combine basic_io and waveform into one tidy table\n",
    "basic_melt = df_basic_io.melt(id_vars=['cell_id','treatment'], value_vars=[c for c in basic_io_cols if c in df_basic_io.columns],\n",
    "                              var_name='variable', value_name='value')\n",
    "wave_melt  = df_waveform.melt(id_vars=['cell_id','treatment'], value_vars=[c for c in waveform_cols if c in df_waveform.columns],\n",
    "                              var_name='variable', value_name='value')\n",
    "all_melt   = pd.concat([basic_melt, wave_melt], ignore_index=True)\n",
    "\n",
    "# catplot (violin + swarm overlay)\n",
    "g = sns.catplot(\n",
    "    data=all_melt.dropna(subset=['value']), col='variable', col_wrap=4, errorbar='se', sharey=False,\n",
    "    x='treatment', y='value', hue='treatment', kind='bar',\n",
    "    height=5, aspect=1.8\n",
    ")\n",
    "# sns.swarmplot(\n",
    "#     data=all_melt.dropna(subset=['value']),\n",
    "#     x='variable', y='value', hue='treatment', dodge=True, size=3, color='k', ax=g.ax\n",
    "# )\n",
    "# g.ax.set_title('Melted Variables (catplot)')\n",
    "# g.ax.set_xlabel('Variable')\n",
    "# g.ax.set_ylabel('Value (mixed units)')\n",
    "# # dedupe legend\n",
    "# handles, labels = g.ax.get_legend_handles_labels()\n",
    "# if handles:\n",
    "#     g.ax.legend(handles[:len(set(labels))], list(dict.fromkeys(labels)))\n",
    "\n",
    "# plt.tight_layout()\n",
    "save_plot_and_data('section7_melted', 'melted_catplot_all_vars', all_df, g.figure, {'note':'Variables may mix units.'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f8a555",
   "metadata": {},
   "source": [
    "### 8. PCA + Logistic Regression ‚Äî Save PCA coordinates & figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4bddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on cell_id & treatment\n",
    "df_merged = pd.merge(\n",
    "    df_basic_io, df_waveform,\n",
    "    on=['cell_id','treatment'], how='inner'\n",
    ")\n",
    "df_merged = df_merged.fillna(df_merged.median(numeric_only=True))\n",
    "\n",
    "features = [c for c in basic_io_cols if c in df_merged.columns] + [c for c in waveform_cols if c in df_merged.columns]\n",
    "X = df_merged[features].values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "y = df_merged[group_col].values\n",
    "\n",
    "pca = PCA(n_components=2, random_state=0)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "log_reg = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "scores = []\n",
    "\n",
    "for train_idx, test_idx in skf.split(X_pca, y):\n",
    "    X_train, X_test = X_pca[train_idx], X_pca[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    y_pred = log_reg.predict(X_test)\n",
    "    scores.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(f\"‚úÖ Mean CV Accuracy: {np.mean(scores):.3f}\")\n",
    "print(f\"üìÑ Fold Accuracies:  {np.round(scores, 3)}\")\n",
    "\n",
    "# Final model\n",
    "log_reg.fit(X_pca, y)\n",
    "\n",
    "# Decision boundary\n",
    "x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\n",
    "y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(x_min, x_max, 500),\n",
    "    np.linspace(y_min, y_max, 500)\n",
    ")\n",
    "Z = log_reg.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1].reshape(xx.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "for tr, col in zip(np.unique(y), ['#1f77b4', '#ff7f0e']):\n",
    "    mask = y == tr\n",
    "    ax.scatter(X_pca[mask, 0], X_pca[mask, 1], label=tr, c=col, edgecolor='k', s=50)\n",
    "ax.contour(xx, yy, Z, levels=[0.5], linestyles='--', colors='gray')\n",
    "ax.set_xlabel('PC 1')\n",
    "ax.set_ylabel('PC 2')\n",
    "ax.set_title(f'PCA + Logistic Regression\\n5-Fold CV Accuracy: {np.mean(scores):.2f}')\n",
    "ax.legend(title=group_col)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save fig + PCA table\n",
    "pca_df = pd.DataFrame({\n",
    "    'cell_id': df_merged['cell_id'],\n",
    "    'treatment': df_merged['treatment'],\n",
    "    'PC1': X_pca[:,0],\n",
    "    'PC2': X_pca[:,1]\n",
    "})\n",
    "save_plot_and_data('pca', 'pca_logreg', pca_df, fig, {'cv_scores': list(map(float, scores))})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72014919",
   "metadata": {},
   "source": [
    "### 9‚Äì12. Chirp & Short-Chirp ‚Äî normalize, band, deltas (all plots saved + CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c034d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_types_ch = ['ch', 'sch']\n",
    "metrics = {'ch': 'impedance', 'sch': 'spike_count'}\n",
    "file_paths = {s: os.path.join(analyzed_path, s, 'features.csv') for s in stim_types_ch}\n",
    "freq_bins = (3, 6, 12, 15, 30, 60)\n",
    "\n",
    "# Containers\n",
    "dfs = {}\n",
    "dfs_binned_raw = {}\n",
    "dfs_binned_norm = {}\n",
    "deltas_raw = {}\n",
    "deltas_norm = {}\n",
    "\n",
    "# Deltas helper\n",
    "def compute_deltas(group, y):\n",
    "    bp = group.set_index('freq_bins')[y]\n",
    "    return pd.Series({\n",
    "        'delta_3-6__6-12':   bp.get(pd.Interval(3, 6),  np.nan) - bp.get(pd.Interval(6, 12), np.nan),\n",
    "        'delta_15-30__3-6':  bp.get(pd.Interval(15, 30), np.nan) - bp.get(pd.Interval(3, 6), np.nan),\n",
    "        'delta_15-30__6-12': bp.get(pd.Interval(15, 30), np.nan) - bp.get(pd.Interval(6, 12), np.nan),\n",
    "    })\n",
    "\n",
    "# Process CH and SCH\n",
    "for stim in stim_types_ch:\n",
    "    metric = metrics[stim]\n",
    "\n",
    "    # Load and clean\n",
    "    df = pd.read_csv(file_paths[stim])\n",
    "    df = df.rename(columns={'file_name': 'cell_id', 'condition_0': 'treatment'})\n",
    "    try:\n",
    "        df = df[~df['cell_id'].isin(to_exclude)].copy()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Remove zero-spike cells if needed\n",
    "    if metric == 'spike_count':\n",
    "        total = df.groupby(['cell_id', 'treatment'])[metric].transform('sum')\n",
    "        df = df[total > 0]\n",
    "\n",
    "    # Normalize per cell√ótreatment\n",
    "    total = df.groupby(['cell_id', 'treatment'])[metric].transform('sum')\n",
    "    norm_col = f'norm_{metric}'\n",
    "    df[norm_col] = df[metric] / total\n",
    "\n",
    "    # Checks\n",
    "    check = df.groupby(['cell_id', 'treatment'])[norm_col].sum().reset_index(name='sum_norm')\n",
    "    print(f\"--- {stim.upper()} normalization check --- min={check['sum_norm'].min():.4f} max={check['sum_norm'].max():.4f}\")\n",
    "\n",
    "    # Bin frequencies\n",
    "    df['freq_bins'] = pd.cut(df['freq'], freq_bins)\n",
    "\n",
    "    dfs[stim] = df\n",
    "\n",
    "    # Binned RAW\n",
    "    df_binned_raw = (\n",
    "        df.groupby(['freq_bins', 'cell_id', 'treatment'], observed=True)[metric]\n",
    "        .mean().reset_index()\n",
    "    )\n",
    "    dfs_binned_raw[stim] = df_binned_raw\n",
    "\n",
    "    deltas_raw[stim] = (\n",
    "        df_binned_raw.groupby(['cell_id', 'treatment'])\n",
    "        .apply(compute_deltas, y=metric)\n",
    "        .reset_index()\n",
    "        .melt(id_vars=['cell_id', 'treatment'], var_name='delta_type', value_name=metric)\n",
    "    )\n",
    "\n",
    "    # Binned NORM\n",
    "    df_binned_norm = (\n",
    "        df.groupby(['freq_bins', 'cell_id', 'treatment'], observed=True)[norm_col]\n",
    "        .mean().reset_index()\n",
    "    )\n",
    "    dfs_binned_norm[stim] = df_binned_norm\n",
    "\n",
    "    deltas_norm[stim] = (\n",
    "        df_binned_norm.groupby(['cell_id', 'treatment'])\n",
    "        .apply(compute_deltas, y=norm_col)\n",
    "        .reset_index()\n",
    "        .melt(id_vars=['cell_id', 'treatment'], var_name='delta_type', value_name=metric)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906d2ebe",
   "metadata": {},
   "source": [
    "#### 9. Raw metric vs frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e933e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stim in stim_types_ch:\n",
    "    metric = metrics[stim]\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    sns.lineplot(data=dfs[stim], x='freq', y=metric, hue='treatment', errorbar='se', marker='o', ax=ax)\n",
    "    ax.set_title(f'{stim.upper()}: Raw {metric.capitalize()} vs Frequency')\n",
    "    ax.set_xlabel('Frequency (Hz)')\n",
    "    ax.set_ylabel(f'{metric} (MŒ©)' if metric=='impedance' else metric)\n",
    "    plt.tight_layout()\n",
    "    save_plot_and_data(f'{stim}', f'{stim}_raw_vs_freq', dfs[stim].copy(), fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f28d93c",
   "metadata": {},
   "source": [
    "#### 10. Normalized metric vs frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97756083",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stim in stim_types_ch:\n",
    "    norm_col = f'norm_{metrics[stim]}'\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    sns.lineplot(data=dfs[stim], x='freq', y=norm_col, hue='treatment', errorbar='se', marker='o', ax=ax)\n",
    "    ax.set_title(f'{stim.UPPER() if hasattr(stim, \"UPPER\") else stim.upper()}: Normalized {metrics[stim].capitalize()} vs Frequency')\n",
    "    ax.set_xlabel('Frequency (Hz)')\n",
    "    ax.set_ylabel(norm_col)\n",
    "    plt.tight_layout()\n",
    "    save_plot_and_data(f'{stim}', f'{stim}_normalized_vs_freq', dfs[stim].copy(), fig, {'norm_col': norm_col})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff88ba",
   "metadata": {},
   "source": [
    "#### 11. Binned Raw & Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e97770",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stim in stim_types_ch:\n",
    "    metric = metrics[stim]\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    sns.barplot(data=dfs_binned_raw[stim], x='freq_bins', y=metric, hue='treatment', errorbar='se', ax=ax)\n",
    "    ax.set_title(f'{stim.upper()}: Binned Raw {metric.capitalize()}')\n",
    "    ax.set_xlabel('Frequency Band (Hz)')\n",
    "    ax.set_ylabel(f'{metric} (MŒ©)' if metric=='impedance' else metric)\n",
    "    plt.tight_layout()\n",
    "    save_plot_and_data(f'{stim}', f'{stim}_binned_raw', dfs_binned_raw[stim].copy(), fig)\n",
    "\n",
    "for stim in stim_types_ch:\n",
    "    norm_col = f'norm_{metrics[stim]}'\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    sns.barplot(data=dfs_binned_norm[stim], x='freq_bins', y=norm_col, hue='treatment', errorbar='se', ax=ax)\n",
    "    ax.set_title(f'{stim.upper()}: Binned Normalized {metrics[stim].capitalize()}')\n",
    "    ax.set_xlabel('Frequency Band (Hz)')\n",
    "    ax.set_ylabel(norm_col)\n",
    "    plt.tight_layout()\n",
    "    save_plot_and_data(f'{stim}', f'{stim}_binned_normalized', dfs_binned_norm[stim].copy(), fig, {'norm_col': norm_col})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc1ab0c",
   "metadata": {},
   "source": [
    "#### 12. Deltas (raw & normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17305e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stim in stim_types_ch:\n",
    "    metric = metrics[stim]\n",
    "    fig, ax = plt.subplots(figsize=(7.5,5))\n",
    "    sns.barplot(data=deltas_raw[stim], x='delta_type', y=metric, hue='treatment', errorbar='se', ax=ax)\n",
    "    ax.set_title(f'{stim.upper()}: Œîs on Raw {metric.capitalize()}')\n",
    "    ax.set_xlabel('Delta Type')\n",
    "    ax.set_ylabel(f'{metric} (MŒ©)' if metric=='impedance' else metric)\n",
    "    plt.tight_layout()\n",
    "    save_plot_and_data(f'{stim}', f'{stim}_deltas_raw', deltas_raw[stim].copy(), fig)\n",
    "\n",
    "for stim in stim_types_ch:\n",
    "    metric = metrics[stim]\n",
    "    fig, ax = plt.subplots(figsize=(7.5,5))\n",
    "    sns.barplot(data=deltas_norm[stim], x='delta_type', y=metric, hue='treatment', errorbar='se', ax=ax)\n",
    "    ax.set_title(f'{stim.upper()}: Œîs on Normalized {metric.capitalize()}')\n",
    "    ax.set_xlabel('Delta Type')\n",
    "    ax.set_ylabel(metric)\n",
    "    plt.tight_layout()\n",
    "    save_plot_and_data(f'{stim}', f'{stim}_deltas_normalized', deltas_norm[stim].copy(), fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ebf560",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- All plots now save **PNG+SVG** + the exact **CSV** used to generate them.\n",
    "- Minimal code changes; your processing logic remains intact.\n",
    "- Units are preserved as in your pipeline (Vm mV, I pA, time ms, Rin MŒ©, etc.)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
